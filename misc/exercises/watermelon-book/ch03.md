### 第三章 线性模型

#### 3.1 试析在什么情况下式（3.2）中不必考虑偏置项$b$

$$
f(x) = w^\top x + b \tag{3.2}
$$

答：通过数学矩阵运算可以消去偏置项。

$$
let \, \dot{x} = [x,b], \dot{w} = [w,1],so \, that \\
f(\dot x) = {\dot w}^\top \dot x  \\ 
$$

#### 3.2 试证明，对于参数$w$,对率回归的目标函数（3.18）是非凸的，但其对数似然函数（3.27）是凸的。

$$
\tag{3.18}\\
y = {(1+e^{ -( w^{\top} \bold{x}+b) } ) } ^{-1}
$$

$$
\tag{3.27}
\ell(\bold{\beta}) = \sum_{i=1} ^m 
\left( { -y_i \beta ^{\top}\hat{x}_i + \ln( 1+ e^{\beta ^{\top}\hat{x}_i})}\right)
$$

答：函数是凸函数，当且仅当满足下面条件：
凸函数是一个定义在某个向量空间的凸子集C上的实值函数f，而且对于凸子集C中任意两个向量$x_1$、$x_2$有

$$
f( (x_1 + x_2)/2) \le (f(x_1) + f(x_2))/2
$$

成立。或者：

若这里凸集C即某个区间I，那么就是：设f为定义在区间I上的函数，若对I上的任意两点$x_1 , x_2$和任意的实数$\lambda \in [0,1] $，总有

$$
f(\lambda x_1+ (1-\lambda x_2 )) \le 
\lambda f(x_1 ) + (1-\lambda)f(x_2)
$$

对实数集上的函数，可通过求二阶导数来判别：若二阶导数在区间上非负，则称为凸函数；若二阶导数在区间上恒大于 0，则称为严格凸函数。

对于多元函数，其Hessian matrix为半正定即为凸函数.

$$
z = w^{\top} \bold{x}+b \\
f = \sigma(z) = \frac{1}{1+e^{-z}} \\
\frac{d\sigma}{dz} = ? \\
\frac {\partial z}{\partial w} = ? \\
\frac {\partial f}{\partial w} = \frac{d\sigma}{dz} \frac {\partial z}{\partial w} = ?  

$$

#### 3.3 编程实现对数回归，并给出西瓜数据集$3.0\alpha$上的结果。

#### 3.4 选择两个UCI数据集，比较10折交叉验证法和留一法所估计出的对数回归的错误率。

注：UCI数据集见http://archive.ics.uci.edu/ml

```textile
答：
http://archive.ics.uci.edu/ml/machine-learning-databases/wine/
http://archive.ics.uci.edu/ml/machine-learning-databases/car/
```

#### 3.5 编程实现线性判别分析，并给出西瓜数据集3.0$\alpha$上的结果。

```python
import numpy as np
import matplotlib.pyplot as plt
data = [[0.697, 0.460, 1],
        [0.774, 0.376, 1],
        [0.634, 0.264, 1],
        [0.608, 0.318, 1],
        [0.556, 0.215, 1],
        [0.403, 0.237, 1],
        [0.481, 0.149, 1],
        [0.437, 0.211, 1],
        [0.666, 0.091, 0],
        [0.243, 0.267, 0],
        [0.245, 0.057, 0],
        [0.343, 0.099, 0],
        [0.639, 0.161, 0],
        [0.657, 0.198, 0],
        [0.360, 0.370, 0],
        [0.593, 0.042, 0],
        [0.719, 0.103, 0]]   #书中89页西瓜数据集
#数据集按瓜好坏分类
data = np.array([i[:-1] for i in data])
X0 = np.array(data[:8])
X1 = np.array(data[8:])
#求正反例均值
miu0 = np.mean(X0, axis=0).reshape((-1, 1))
miu1 = np.mean(X1, axis=0).reshape((-1, 1))
#求协方差
cov0 = np.cov(X0, rowvar=False)
cov1 = np.cov(X1, rowvar=False)
#求出w
S_w = np.mat(cov0 + cov1)
Omiga = S_w.I * (miu0 - miu1)
#画出点、直线
plt.scatter(X0[:, 0], X0[:, 1], c='b', label='+', marker = '+')
plt.scatter(X1[:, 0], X1[:, 1], c='r', label='-', marker = '_')
plt.plot([0, 1], [0, -Omiga[0] / Omiga[1]], label='y')
plt.xlabel('密度', fontproperties='SimHei', fontsize=15, color='green');
plt.ylabel('含糖率', fontproperties='SimHei', fontsize=15, color='green');
plt.title(r'3.5 线性判别分析', fontproperties='SimHei', fontsize=25);
plt.legend()
plt.show()
```

#### 3.6 线性判别法仅在线性可分数据集上获得理想结果，试设计一个改进方法，使其能较好地用于非线性可分数据.

```
答：能想到的是升到高维，使之能线性可分。
```

#### 3.7 令码长为9，类别数为4，试给出海明距离意义下理论最优的ECOC二元码并证明之。

```
答：
```

#### $\bold{3.8} ^*$ ECOC编码能起到理想纠错作用的重要条件是：在每一位编码上出错的概率相当且独立。试析多分类任务经ECOC编码后产生的二类分类器满足该条件的可能性及由此产生的影响。

#### 3.9 使用OvR和MvM将多分类任务分解为二分类任务求解时，试述为何无需专门针对类别不平衡性进行处理

#### $\bold{3.10^*}$ 试推导出多分类代价敏感学习（仅考虑基于类别的误分类代价）使用“再缩放”能获得理论最优解的条件。

#### 
