### 第十章 降维与度量学习

#### 10.1 编程实现k近邻分类器，在西瓜数据集$3.0\alpha$上比较其分类边界与决策树分类边界之间的异同。

#### 10.2 令err、err*分别表示最近邻分类器与贝叶斯最优分类器的期望错误率，试证明

$$
err^* \le err \le 
err^* \left( 
2 - \frac{|\mathcal{y}|} {|\mathcal{y}| -1 } \times err^*
\right).
$$

#### 10.3 在对高维数据降维之前应先进行“中心化”，常见的是将协方差矩阵$XX^\top$ 转化为$XHH^{\top}X^{\top}$,其中$H=I-\frac{1}{m}ll^{\top}$，试析其效果。

#### 10.4 在实践中，协方差矩阵$XX^T$的特征值分解常由中心化后的样本矩阵$X$的奇异值分解代替，试述其原因。

#### 10.5 降维中涉及的投影矩阵通常要求是正交的。试述正交、非正交投影矩阵用于降维的优缺点。

#### 10.6 试使用MATLAB中的PCA函数对Yale人脸数据集进行降维，并观察前20个特征向量所对应的图像。

#### 10.7 试述核化线性降维与流行学习之间的联系及优缺点。

#### 10.8* k近邻图和$\epsilon$近邻图存在的短路和断路问题会给Isomap造成困扰，试设计一个方法缓解该问题。

#### 10.9* 试设计一个方法为新样本找到LLE降维后的低维坐标。

#### 10.10 试述如何确保度量学习产生的距离能满足距离度量的四条基本性质。
