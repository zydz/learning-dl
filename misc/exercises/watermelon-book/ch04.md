### 第四章 决策树

#### 4.1 试证明对于不含有冲突数据（即特征向量完全相同但标记不同）的训练集，必存在与训练集一致（即训练误差为0）的决策树。

#### 4.2 试分析使用“最小训练误差”作为决策树划分选择准则的缺陷。

#### 4.3 试编程实现基于信息熵进行划分选择的决策树算法，并为表4.3中的数据生成一颗决策树。

#### 4.4 试编程实现基于基尼指数进行划分的决策树算法，为表4.2中数据生成预剪枝、后剪枝决策树，并与未剪枝决策树进行比较。

#### 4.5 试编程实现基于对率回归进行划分选择的决策树算法，并为表4.3中数据生成一颗决策树。

#### 4.6 试选择4个UCI数据集，对上述3种算法所产生的未剪枝、预剪枝、后剪枝决策树进行实验比较，并进行适当的统计显著性检验。

#### 4.7 图4.2是一个递归算法，若面临巨量数据，则决策树的层数会很深，使用递归方法易导致“栈”溢出。试使用“队列”数据结构，以参数MaxDepth控制树的最大深度，写出与图4.2等价、但不使用递归的决策树生成算法。

#### 4.8* 试将决策树生成的深度优先搜索过程修改为广度优先搜索，以参数MaxNode控制树的最大节点数，将题4.7中基于队列的决策树算法进行改写。对比题4.7中的算法，试析那种方法更易于控制决策树所需存储不超出内存。

#### 4.9 试将4.4.2节对缺失值的处理机制推广到基尼指数的计算中。

#### 4.10 从网上下载或自己编程实现任意一种多变量决策树算法，并观察其在西瓜数据集3.0上产生的结果。
