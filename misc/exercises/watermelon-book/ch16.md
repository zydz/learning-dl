### 第16章 强化学习



#### 16.1 用于K-摇臂赌博机的UCB(Upper Confidence Bound，上置信界)方法每次选择$Q(k)+UC(k)$最大的摇臂，其中$Q(K)$为摇臂k当前的平均奖赏，UC(k)为置信区间。例如

$$
Q(k) + \sqrt{\frac{2\ln n}{n_k}}
$$

其中$n$为已执行所有摇臂的总次数，$n_k$为已执行摇臂$k$的次数。试比较UCB方法与$\epsilon$贪心法和Softmax方法的异同。



#### 16.2 借鉴图16.7，试写出基于$\gamma$折扣奖赏函数的策略评估算法。



#### 16.3 借鉴图16.8，试写出基于$\gamma$折扣奖赏函数的策略迭代算法。



#### 16.4 在没有MDP模型时，可以先学习MDP模型(例如使用随机策略进行采样，从样本中估计出转移函数和奖赏函数)，然后再使用有模型强化学习方法。试述该方法与免模型强化学习方法的优缺点。



#### 16.5 试推导出Sarsa算法的更新公式(16.31)。



#### 16.6 试借鉴图16.14给出线性值函数近似Q-学习算法。



#### 16.7 线性值函数近似在实践中往往有较大的误差。试结合BP神经网络，将线性值函数近似Sarsa算法推广为使用神经网络近似的Sarsa算法。



#### 16.8 试结合核函数，将线性值函数近似Sarsa算法推广为使用核函数的非线性值函数近似Sarsa算法。



#### 16.9 对于目标驱动(goal-directed)的强化学习任务，目标是到达某一状态，例如将汽车驾驶到预定位置。试为这样的任务设置奖赏函数，并讨论不同奖赏函数饿作用(例如每一步未达目标的奖赏为0、-1、1)。



#### 16.10* 与传统监督学习不同，直接模仿学习在不同时刻所面临的数据分布可能不同。试设计一个考虑不同时刻数据分布变化的模拟学习算法。






