### 第八章 集成学习

#### 8.1 假设抛硬币正面朝上的概率是$p$，反面朝上的概率是$1-p$. 令$H(n)$代表抛n次硬币所得的正面朝上的次数，则最多k次正面朝上的概率为

$$
P(H(n)<k) = \sum _{i=0} ^k \dbinom{n}{i} p^i(1-p)^{n-1}. \tag{8.43}
$$

对$\delta > 0, k=(p-\delta)n$，有Hoeffding不等式

$$
P(H(n)<(p-\delta)n) \le e^{-2\delta^2 n} \tag{8.44}
$$

试推导出式(8.3)。

$$
\bot \lVert \rVert \lrcorner\llcorner \Vdash \coprod 
$$

#### 8.2 对于0/1损失函数来说，指数损失函数并非仅有的一致替代函数。考虑式(8.5)，试证明：任意损失函数$\ell(-f(x))H(x)$，若对于$H(x)$在区间$[-\infty, \delta] (\delta>0)$上单调递减，则$\ell$是0/1损失函数的一致替代函数。

#### 8.3 从网上下载或自己编程实现AdaBoost，以不剪枝决策树为基学习器，在西瓜数据集$3.0\alpha$上训练一个AdaBoost集成，并与图8.4进行比较。

#### 8.4 GradientBoosting [Friedman, 2001]是一种常用的Boosting算法，试析其与AdaBoost的异同。

#### 8.5 试编程实现Bagging， 以决策树桩为基学习器，在西瓜数据集$3.0\alpha$上训练一个Bagging集成，并与图$8.6$进行比较。

#### 8.6 试析Bagging通常为何难以提升朴素贝叶斯分类器的性能。

#### 8.7 试析随机森林为何比决策树的Bagging集成的训练速度更快。

#### 8.8 MultiBoosting算法 [Webb, 2000]将AdaBoost作为Bagging的基学习器，Iterative Bagging 算法 [Breiman, 2001b] 则是将Bagging作为AdaBoost的基学习器，试比较二者的优缺点。

#### 8.9* 试设计一种可视的多样性度量，对习题8.3和习题8.5中得到的集成进行评估，并与$\kappa -$误差图进行比较。

#### 8.10* 试设计一种能提升k近邻分类器性能的集成学习算法。
